{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus: Europarl v7 (http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz)\n",
    "\n",
    "Using:\n",
    "* Python 3.8.3\n",
    "* TensorFlow 2.2.0\n",
    "* SentencePiece 0.1.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Tokenizer\n",
    "import sentencepiece as sp\n",
    "# Data preprocessing tools\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# General Keras functionalities\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Neural network components from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source text database\n",
    "data_filename = \"./fra.txt\"\n",
    "\n",
    "# Text preprocessing\n",
    "subset_size = 10000 # total sentence pairs in dataset: 175,623\n",
    "test_size = 1000 # sentence pairs for testing\n",
    "train_size = subset_size - test_size # sentence pairs for training\n",
    "clean_en_filename = \"clean_en.txt\" # file to save cleaned English text data\n",
    "clean_fr_filename = \"clean_fr.txt\" # French\n",
    "clean_train_filename = \"clean_en_fr.txt\" # both languages\n",
    "\n",
    "# Let's define vocabulary size for out tokenizer\n",
    "vocab_size = 2000\n",
    "# We'll use the same size for both languages to simplify\n",
    "en_vocab_size = vocab_size\n",
    "fr_vocab_size = vocab_size\n",
    "# max_sentence_length = 20 # Pad all sentences to 40 word pieces (tokens) max\n",
    "\n",
    "# Defining all the parameters of our network\n",
    "nb_cells = 256 # LSTM cells in encoder/decoder\n",
    "\n",
    "# Training parameters\n",
    "nb_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "# File name to save our trained model weights\n",
    "trained_model_filename = \"fr_en_nmt_model_test.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the text dataset file\n",
    "file = open(\n",
    "    data_filename,\n",
    "    mode = 'rt',\n",
    "    encoding = 'utf-8')\n",
    "# Getting the text content\n",
    "raw_text = file.read()\n",
    "# Closing the file handle\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
      "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (g\n"
     ]
    }
   ],
   "source": [
    "# Checking the beginning of the text\n",
    "print(raw_text[:256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing: cleaning and reducing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset in a Pandas dataframe\n",
    "train_df = pd.read_csv(\n",
    "    data_filename, # path to our dataset file\n",
    "    sep='\\t', # tab delimiter between columns in the csv\n",
    "    usecols=[0, 1], # import only columns 0 and 1\n",
    "    nrows=subset_size, # read only the first subset_size rows\n",
    "    names=[\"en\",\"fr\"]) # label them 'en' and 'fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    en                                fr\n",
      "0                  Go.                              Va !\n",
      "1                  Hi.                           Salut !\n",
      "2                  Hi.                            Salut.\n",
      "3                 Run!                           Cours !\n",
      "4                 Run!                          Courez !\n",
      "...                ...                               ...\n",
      "9995  Be more precise.                 Soit plus précis.\n",
      "9996  Be quiet, girls.  Restez tranquilles, les filles !\n",
      "9997  Be very careful.               Sois très prudent !\n",
      "9998  Be very careful.              Soyez très prudent !\n",
      "9999  Be very careful.             Soyez très prudente !\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check our dataframe\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional cleanup here\n",
    "# TODO: maybe lowercase all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our cleaned and reduced dataset\n",
    "train_df.to_csv(\n",
    "    clean_train_filename,\n",
    "    sep='\\t', # using tab separators\n",
    "    index=False) # don't print the row index in the csv\n",
    "\n",
    "# Saving the English part separately for SentencePiece\n",
    "train_df.to_csv(\n",
    "    clean_en_filename,\n",
    "    columns=['en'], # print only the column 'en'\n",
    "    index=False)\n",
    "\n",
    "# And the French one\n",
    "train_df.to_csv(\n",
    "    clean_fr_filename,\n",
    "    columns=['fr'], # print only the column 'fr'\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for English\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input = clean_en_filename,\n",
    "    model_prefix = 'en',\n",
    "    vocab_size = en_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for French\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input = clean_fr_filename,\n",
    "    model_prefix = 'fr',\n",
    "    vocab_size = fr_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tokenizer object for English\n",
    "en_sp = sp.SentencePieceProcessor()\n",
    "# Loading the English model\n",
    "en_sp.Load(\"en.model\")\n",
    "# Creating a tokenizer object for French\n",
    "fr_sp = sp.SentencePieceProcessor()\n",
    "# Loading the French model\n",
    "fr_sp.Load(\"fr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', '▁like', '▁gree', 'n', '▁a', 'pples', '.']\n",
      "[4, 41, 1033, 24, 10, 1235, 3]\n",
      "I like green apples.\n",
      "['▁J', \"'\", 'aime', '▁le', 's', '▁pommes', '▁vert', 'es', '.']\n",
      "[11, 4, 79, 20, 5, 1449, 1155, 63, 3]\n",
      "J'aime les pommes vertes.\n"
     ]
    }
   ],
   "source": [
    "# Testing the English tokenizer\n",
    "en_test_sentence = \"I like green apples.\"\n",
    "# Encoding pieces\n",
    "print(en_sp.EncodeAsPieces(en_test_sentence))\n",
    "# Encoding pieces as IDs\n",
    "print(en_sp.EncodeAsIds(en_test_sentence))\n",
    "# Decoding encoded IDs\n",
    "print(en_sp.DecodeIds(en_sp.EncodeAsIds(en_test_sentence)))\n",
    "\n",
    "# Testing the French tokenizer\n",
    "fr_test_sentence = \"J'aime les pommes vertes.\"\n",
    "# Encoding pieces\n",
    "print(fr_sp.EncodeAsPieces(fr_test_sentence))\n",
    "# Encoding pieces as IDs\n",
    "print(fr_sp.EncodeAsIds(fr_test_sentence))\n",
    "# Decoding encoded IDs\n",
    "print(fr_sp.DecodeIds(fr_sp.EncodeAsIds(fr_test_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained tokenizer for English and French\n",
    "# Creating a tokenizer object for English\n",
    "en_sp = sp.SentencePieceProcessor()\n",
    "# Loading the English model\n",
    "en_sp.Load(\"en.model\")\n",
    "# Creating a tokenizer object for French\n",
    "fr_sp = sp.SentencePieceProcessor()\n",
    "# Loading the French model\n",
    "fr_sp.Load(\"fr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned up dataset\n",
    "train_df = pd.read_csv(\n",
    "    clean_train_filename,\n",
    "    sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    en                                fr\n",
      "0                  Go.                              Va !\n",
      "1                  Hi.                           Salut !\n",
      "2                  Hi.                            Salut.\n",
      "3                 Run!                           Cours !\n",
      "4                 Run!                          Courez !\n",
      "...                ...                               ...\n",
      "9995  Be more precise.                 Soit plus précis.\n",
      "9996  Be quiet, girls.  Restez tranquilles, les filles !\n",
      "9997  Be very careful.               Sois très prudent !\n",
      "9998  Be very careful.              Soyez très prudent !\n",
      "9999  Be very careful.             Soyez très prudente !\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Checking the resulting data frame\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize our text (list of sentences) and\n",
    "# add it to our data frame in the column 'label'\n",
    "def tokenize_text(df, spm, txt_label, id_label):\n",
    "    ids = []\n",
    "    for line in df[txt_label].tolist():\n",
    "        id_line = spm.EncodeAsIds(line)\n",
    "        ids.append(id_line)\n",
    "    df[id_label] = ids\n",
    "\n",
    "# Let's run this function on the English text\n",
    "tokenize_text(train_df, en_sp, 'en', 'en_ids')\n",
    "# And on the French text\n",
    "tokenize_text(train_df, fr_sp, 'fr', 'fr_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    en                                fr  \\\n",
      "0                  Go.                              Va !   \n",
      "1                  Hi.                           Salut !   \n",
      "2                  Hi.                            Salut.   \n",
      "3                 Run!                           Cours !   \n",
      "4                 Run!                          Courez !   \n",
      "...                ...                               ...   \n",
      "9995  Be more precise.                 Soit plus précis.   \n",
      "9996  Be quiet, girls.  Restez tranquilles, les filles !   \n",
      "9997  Be very careful.               Sois très prudent !   \n",
      "9998  Be very careful.              Soyez très prudent !   \n",
      "9999  Be very careful.             Soyez très prudente !   \n",
      "\n",
      "                      en_ids                               fr_ids  \n",
      "0                    [81, 3]                             [199, 9]  \n",
      "1                  [1004, 3]                             [992, 9]  \n",
      "2                  [1004, 3]                             [992, 3]  \n",
      "3                  [472, 18]                      [18, 812, 5, 9]  \n",
      "4                  [472, 18]                     [18, 812, 49, 9]  \n",
      "...                      ...                                  ...  \n",
      "9995  [42, 320, 282, 919, 3]              [118, 22, 203, 1143, 3]  \n",
      "9996   [42, 271, 89, 753, 3]  [6, 221, 265, 5, 66, 20, 5, 765, 9]  \n",
      "9997   [42, 17, 151, 177, 3]            [118, 5, 58, 169, 361, 9]  \n",
      "9998   [42, 17, 151, 177, 3]           [108, 49, 58, 169, 361, 9]  \n",
      "9999   [42, 17, 151, 177, 3]           [108, 49, 58, 169, 375, 9]  \n",
      "\n",
      "[10000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Checking the resulting data frame\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tokenized English sentence length\n",
    "en_max_len = max(len(line) for line in train_df['en_ids'].tolist())\n",
    "# Check tokenized French sentence length\n",
    "fr_max_len = max(len(line) for line in train_df['fr_ids'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English maximum sentence length: 10\n",
      "French maximum sentence length: 22\n"
     ]
    }
   ],
   "source": [
    "print(\"English maximum sentence length:\", en_max_len)\n",
    "print(\"French maximum sentence length:\", fr_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence padding\n",
    "# Pad English tokens\n",
    "padded_en_ids = pad_sequences(\n",
    "    train_df['en_ids'].tolist(),\n",
    "    maxlen = en_max_len,\n",
    "    padding = 'post')\n",
    "# Add them to our training data frame\n",
    "train_df['pad_en_ids'] = padded_en_ids.tolist()\n",
    "\n",
    "# Pad French tokens\n",
    "padded_fr_ids = pad_sequences(\n",
    "    train_df['fr_ids'].tolist(),\n",
    "    maxlen = fr_max_len,\n",
    "    padding = 'post')\n",
    "# Add them to our training data frame\n",
    "train_df['pad_fr_ids'] = padded_fr_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [199, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "1       [992, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "2       [992, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "3       [18, 812, 5, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4       [18, 812, 49, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
      "                              ...                        \n",
      "9995    [118, 22, 203, 1143, 3, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "9996    [6, 221, 265, 5, 66, 20, 5, 765, 9, 0, 0, 0, 0...\n",
      "9997    [118, 5, 58, 169, 361, 9, 0, 0, 0, 0, 0, 0, 0,...\n",
      "9998    [108, 49, 58, 169, 361, 9, 0, 0, 0, 0, 0, 0, 0...\n",
      "9999    [108, 49, 58, 169, 375, 9, 0, 0, 0, 0, 0, 0, 0...\n",
      "Name: pad_fr_ids, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df['pad_fr_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling our dataframe around\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [14, 27, 58, 169, 1108, 3, 0, 0, 0, 0, 0, 0, 0...\n",
      "1       [7, 51, 4, 79, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "2       [7, 133, 756, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
      "3       [14, 27, 400, 32, 201, 3, 0, 0, 0, 0, 0, 0, 0,...\n",
      "4       [1281, 45, 8, 40, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "                              ...                        \n",
      "9995    [18, 4, 15, 58, 211, 473, 3, 0, 0, 0, 0, 0, 0,...\n",
      "9996    [520, 45, 34, 177, 9, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9997    [21, 167, 728, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "9998    [7, 1202, 5, 34, 870, 3, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9999    [39, 22, 8, 41, 1951, 10, 0, 0, 0, 0, 0, 0, 0,...\n",
      "Name: pad_fr_ids, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df['pad_fr_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our training input and target output numpy array to feed to our NMT model\n",
    "# We'll take the first train_size lines for training (after random shuffle)\n",
    "trainX = np.asarray(train_df['pad_fr_ids'][0:train_size].tolist())\n",
    "trainY = np.asarray(train_df['pad_en_ids'][0:train_size].tolist())\n",
    "# Reshape the output to match expected dimensionality\n",
    "trainY = trainY.reshape(trainY.shape[0], trainY.shape[1], 1)\n",
    "\n",
    "# The test dataset for checking on the last test_size lines (after random shuffle)\n",
    "testX = np.asarray(train_df['pad_fr_ids'][train_size:].tolist())\n",
    "testY = np.asarray(train_df['pad_en_ids'][train_size:].tolist())\n",
    "# Reshape the output to match expected dimensionality\n",
    "testY = testY.reshape(testY.shape[0], testY.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 22)\n",
      "(9000, 10, 1)\n",
      "(1000, 22)\n",
      "(1000, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training our NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 22, 256)           512000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 10, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10, 2000)          514000    \n",
      "=================================================================\n",
      "Total params: 2,076,624\n",
      "Trainable params: 2,076,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Creating a Keras Sequential object for our NMT model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer to map our one-hot encoding to a small word space\n",
    "model.add(Embedding(\n",
    "    fr_vocab_size,\n",
    "    nb_cells,\n",
    "    input_length = fr_max_len,\n",
    "    mask_zero = True))\n",
    "# Adding an LSTM layer to act as the encoder\n",
    "model.add(LSTM(\n",
    "    units = nb_cells,\n",
    "    return_sequences = False))\n",
    "# Since we are not returning a sequence but just a vector, we need\n",
    "# to repeat this vector multiple times to input it to our decoder LSTM\n",
    "model.add(RepeatVector(en_max_len))\n",
    "# Adding an LSTM layer to act as the decoder\n",
    "model.add(LSTM(\n",
    "    units = nb_cells,\n",
    "    return_sequences = True))\n",
    "# Adding a softmax\n",
    "model.add((Dense(\n",
    "    en_vocab_size,\n",
    "    activation = 'softmax')))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luc/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 3.2775 - val_loss: 2.5726\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.57256, saving model to model.h5\n",
      "Epoch 2/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 2.4382 - val_loss: 2.3756\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.57256 to 2.37557, saving model to model.h5\n",
      "Epoch 3/30\n",
      "9000/9000 [==============================] - 37s 4ms/step - loss: 2.2690 - val_loss: 2.2488\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.37557 to 2.24880, saving model to model.h5\n",
      "Epoch 4/30\n",
      "9000/9000 [==============================] - 37s 4ms/step - loss: 2.1242 - val_loss: 2.1482\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.24880 to 2.14819, saving model to model.h5\n",
      "Epoch 5/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.9946 - val_loss: 2.0573\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.14819 to 2.05727, saving model to model.h5\n",
      "Epoch 6/30\n",
      "9000/9000 [==============================] - 37s 4ms/step - loss: 1.8717 - val_loss: 1.9703\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.05727 to 1.97027, saving model to model.h5\n",
      "Epoch 7/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.7762 - val_loss: 1.9222\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.97027 to 1.92219, saving model to model.h5\n",
      "Epoch 8/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.6904 - val_loss: 1.8633\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.92219 to 1.86327, saving model to model.h5\n",
      "Epoch 9/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 1.6144 - val_loss: 1.8207\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.86327 to 1.82075, saving model to model.h5\n",
      "Epoch 10/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 1.5482 - val_loss: 1.7825\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.82075 to 1.78247, saving model to model.h5\n",
      "Epoch 11/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 1.4829 - val_loss: 1.7498\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.78247 to 1.74978, saving model to model.h5\n",
      "Epoch 12/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.4170 - val_loss: 1.7088\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.74978 to 1.70879, saving model to model.h5\n",
      "Epoch 13/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.3540 - val_loss: 1.6712\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.70879 to 1.67125, saving model to model.h5\n",
      "Epoch 14/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.2920 - val_loss: 1.6466\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.67125 to 1.64657, saving model to model.h5\n",
      "Epoch 15/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.2295 - val_loss: 1.6314\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.64657 to 1.63143, saving model to model.h5\n",
      "Epoch 16/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.1723 - val_loss: 1.5966\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.63143 to 1.59665, saving model to model.h5\n",
      "Epoch 17/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.1136 - val_loss: 1.5708\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.59665 to 1.57078, saving model to model.h5\n",
      "Epoch 18/30\n",
      "9000/9000 [==============================] - 40s 4ms/step - loss: 1.0600 - val_loss: 1.5573\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.57078 to 1.55732, saving model to model.h5\n",
      "Epoch 19/30\n",
      "9000/9000 [==============================] - 40s 4ms/step - loss: 1.0041 - val_loss: 1.5273\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.55732 to 1.52729, saving model to model.h5\n",
      "Epoch 20/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 0.9530 - val_loss: 1.5115\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.52729 to 1.51151, saving model to model.h5\n",
      "Epoch 21/30\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 0.9051 - val_loss: 1.5020\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.51151 to 1.50200, saving model to model.h5\n",
      "Epoch 22/30\n",
      "9000/9000 [==============================] - 41s 5ms/step - loss: 0.8508 - val_loss: 1.4747\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.50200 to 1.47474, saving model to model.h5\n",
      "Epoch 23/30\n",
      "9000/9000 [==============================] - 40s 4ms/step - loss: 0.8094 - val_loss: 1.4758\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.47474\n",
      "Epoch 24/30\n",
      "9000/9000 [==============================] - 41s 5ms/step - loss: 0.7620 - val_loss: 1.4596\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.47474 to 1.45958, saving model to model.h5\n",
      "Epoch 25/30\n",
      "9000/9000 [==============================] - 41s 5ms/step - loss: 0.7223 - val_loss: 1.4581\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.45958 to 1.45808, saving model to model.h5\n",
      "Epoch 26/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 0.6815 - val_loss: 1.4372\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.45808 to 1.43718, saving model to model.h5\n",
      "Epoch 27/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 0.6377 - val_loss: 1.4305\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.43718 to 1.43049, saving model to model.h5\n",
      "Epoch 28/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 0.5998 - val_loss: 1.4176\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.43049 to 1.41762, saving model to model.h5\n",
      "Epoch 29/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 0.5647 - val_loss: 1.4092\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.41762 to 1.40917, saving model to model.h5\n",
      "Epoch 30/30\n",
      "9000/9000 [==============================] - 49s 5ms/step - loss: 0.5324 - val_loss: 1.4066\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.40917 to 1.40663, saving model to model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7feb05b71d00>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model_filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_filename,\n",
    "    monitor = 'val_loss',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    mode = 'min')\n",
    "model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs = nb_epochs,\n",
    "    batch_size = batch_size,\n",
    "    callbacks = [checkpoint],\n",
    "    validation_data = (testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the trained model\n",
    "model = load_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Je suis presque mort. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "I almost died. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "I'm dead. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Es-tu sûre ? ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "Are you sure? ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "Are you sure? ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Soyez de nouveau le bienvenu ! ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "Welcome back. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "Go him. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Allez le chercher ! ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "Go get it. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "Go take it it ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "C'est à moi. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "That is mine. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "It's mine. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Arrête de pleurer ! ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "Stop crying. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "Stop crying. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Est-ce que Tom va bien ? ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "Is Tom well? ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "Is Tom OK? ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Permettez-moi d'essayer. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "Let me try. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "Allow me me me. ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Je me dépêcherai. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "I'll hurry. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "I'll manage. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Veuillez venir. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "Please come. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "Please come. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(testX)\n",
    "\n",
    "# Check the translation on a few sentences\n",
    "decoded_predictions = []\n",
    "for index in range(10):\n",
    "    print(\"Original:\")\n",
    "    print(fr_sp.DecodeIds(testX[index, :].tolist()))\n",
    "    print(\"Expected:\")\n",
    "    print(en_sp.DecodeIds(testY[index, :, 0].tolist()))\n",
    "    print(\"Predicted:\")\n",
    "    print(en_sp.DecodeIds(predictions[index, :].tolist()))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
