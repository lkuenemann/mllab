{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus: Europarl v7 (http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz)\n",
    "\n",
    "Using:\n",
    "* Python 3.8.3\n",
    "* TensorFlow 2.2.0\n",
    "* SentencePiece 0.1.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# General modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Tokenizer\n",
    "import sentencepiece as sp\n",
    "# Data preprocessing tools\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# General Keras functionalities\n",
    "from keras.models import load_model\n",
    "# Neural network components from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source text database\n",
    "en_filename = \"./training/europarl-v7.fr-en.en\"\n",
    "fr_filename = \"./training/europarl-v7.fr-en.fr\"\n",
    "\n",
    "# Text preprocessing\n",
    "max_count = 100000 # total sentence pairs in dataset: 2,007,723\n",
    "clean_en_filename = \"clean_en.txt\" # file to save cleaned English text data\n",
    "clean_fr_filename = \"clean_fr.txt\" # French\n",
    "\n",
    "# Let's define vocabulary size for out tokenizer\n",
    "vocab_size = 10000 # max possible in sentencepiece is 27,535\n",
    "# We'll use the same size for both languages to simplify\n",
    "en_vocab_size = vocab_size\n",
    "fr_vocab_size = vocab_size\n",
    "max_sentence_length = 100 # Pad all sentences to 100 word pieces (tokens) max\n",
    "\n",
    "# Defining all the parameters of our network\n",
    "nb_cells = 1024 # LSTM cells in encoder/decoder\n",
    "\n",
    "# Training parameters\n",
    "nb_epochs = 15\n",
    "batch_size = 64\n",
    "\n",
    "# File name to save our trained model weights\n",
    "trained_model_filename = \"fr_en_nmt_model_test.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the English transcript file\n",
    "en_file = open(\n",
    "    en_filename,\n",
    "    mode = 'rt',\n",
    "    encoding = 'utf-8')\n",
    "# Getting the text content\n",
    "en_text = en_file.read()\n",
    "# Closing the file handle\n",
    "en_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the French transcript file\n",
    "fr_file = open(\n",
    "    fr_filename,\n",
    "    mode = 'rt',\n",
    "    encoding = 'utf-8')\n",
    "# Getting the text content\n",
    "fr_text = fr_file.read()\n",
    "# Closing the file handle\n",
    "fr_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption of the session\n",
      "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I w\n",
      "Reprise de la session\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dern\n"
     ]
    }
   ],
   "source": [
    "# Checking the beginning of both texts\n",
    "print(en_text[:128])\n",
    "print(fr_text[:128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data is all in order. However, I am going to use a small subset of the data to speed up testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing: cleaning and reducing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: extracting the 3 first lines\n",
    "lines = 0\n",
    "en_subtext = []\n",
    "with open(en_filename, 'rt') as en_file:\n",
    "    for line in en_file:\n",
    "        lines += 1\n",
    "        en_subtext.append(line)\n",
    "        if(lines >= 3):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption of the session\\n', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\n', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(en_subtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check how many lines are in the whole text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007723\n",
      "2007723\n"
     ]
    }
   ],
   "source": [
    "lines = 0\n",
    "with open(en_filename, 'rt') as en_file:\n",
    "    for line in en_file:\n",
    "        lines += 1\n",
    "\n",
    "print(lines)\n",
    "\n",
    "lines = 0\n",
    "with open(fr_filename, 'rt') as fr_file:\n",
    "    for line in fr_file:\n",
    "        lines += 1\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a well aligned dataset already. Let's extract the first 10,000 lines to make this training tasks shorter for testing during our development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the 10,000 first lines\n",
    "lines = 0\n",
    "en_subtext = []\n",
    "\n",
    "with open(en_filename, 'rt') as en_file:\n",
    "    for line in en_file:\n",
    "        lines += 1\n",
    "        en_subtext.append(line)\n",
    "        if(lines >= max_count):\n",
    "            break\n",
    "\n",
    "lines = 0\n",
    "fr_subtext = []\n",
    "\n",
    "with open(fr_filename, 'rt') as fr_file:\n",
    "    for line in fr_file:\n",
    "        lines += 1\n",
    "        fr_subtext.append(line)\n",
    "        if(lines >= max_count):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create some text files based on this subtext, since our tokenizer directly takes a file path as input for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up!\n",
    "#TODO: lowercase all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our reduced English dataset\n",
    "with open(clean_en_filename, 'w') as en_file:\n",
    "    for line in en_subtext:\n",
    "        en_file.write(line)\n",
    "\n",
    "# And the French one\n",
    "with open(clean_fr_filename, 'w') as fr_file:\n",
    "    for line in fr_subtext:\n",
    "        fr_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for English\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input = clean_en_filename,\n",
    "    model_prefix = 'en',\n",
    "    vocab_size = en_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for French\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input = clean_fr_filename,\n",
    "    model_prefix = 'fr',\n",
    "    vocab_size = fr_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tokenizer object for English\n",
    "en_sp = sp.SentencePieceProcessor()\n",
    "# Loading the English model\n",
    "en_sp.Load(\"en.model\")\n",
    "# Creating a tokenizer object for French\n",
    "fr_sp = sp.SentencePieceProcessor()\n",
    "# Loading the French model\n",
    "fr_sp.Load(\"fr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', '▁like', '▁green', '▁a', 'p', 'ple', 's', '.']\n",
      "[13, 66, 3745, 10, 315, 6333, 15, 5]\n",
      "I like green apples.\n",
      "['▁J', \"'\", 'aime', '▁les', '▁p', 'omme', 's', '▁verte', 's', '.']\n",
      "[141, 3, 7161, 14, 563, 5964, 11, 5926, 11, 6]\n",
      "J'aime les pommes vertes.\n"
     ]
    }
   ],
   "source": [
    "# Testing the English tokenizer\n",
    "en_test_sentence = \"I like green apples.\"\n",
    "# Encoding pieces\n",
    "print(en_sp.EncodeAsPieces(en_test_sentence))\n",
    "# Encoding pieces as IDs\n",
    "print(en_sp.EncodeAsIds(en_test_sentence))\n",
    "# Decoding encoded IDs\n",
    "print(en_sp.DecodeIds(en_sp.EncodeAsIds(en_test_sentence)))\n",
    "\n",
    "# Testing the French tokenizer\n",
    "fr_test_sentence = \"J'aime les pommes vertes.\"\n",
    "# Encoding pieces\n",
    "print(fr_sp.EncodeAsPieces(fr_test_sentence))\n",
    "# Encoding pieces as IDs\n",
    "print(fr_sp.EncodeAsIds(fr_test_sentence))\n",
    "# Decoding encoded IDs\n",
    "print(fr_sp.DecodeIds(fr_sp.EncodeAsIds(fr_test_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our tokenizer is ready, it is time to prepare our training data properly to feed it to our NMT model. This means creating a pandas data frame with the tokenized (machine readable) sentences for both the source language and the target language.\n",
    "\n",
    "We will also add the human readable sentences, mainly for visually checking the result of the translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained tokenizer for English and French\n",
    "# Creating a tokenizer object for English\n",
    "en_sp = sp.SentencePieceProcessor()\n",
    "# Loading the English model\n",
    "en_sp.Load(\"en.model\")\n",
    "# Creating a tokenizer object for French\n",
    "fr_sp = sp.SentencePieceProcessor()\n",
    "# Loading the French model\n",
    "fr_sp.Load(\"fr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned up dataset\n",
    "# Opening the cleaned-up English text file\n",
    "en_text = []\n",
    "with open(\n",
    "    clean_en_filename,\n",
    "    mode = 'rt',\n",
    "    encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            en_text.append(line)\n",
    "# Opening the cleaned-up French text file\n",
    "fr_text = []\n",
    "with open(\n",
    "    clean_fr_filename,\n",
    "    mode = 'rt',\n",
    "    encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            fr_text.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty data frame\n",
    "training_df = pd.DataFrame()\n",
    "# Add a French text data column\n",
    "training_df['fr'] = fr_text\n",
    "# Add an Eglish text data column\n",
    "training_df['en'] = en_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      fr  \\\n",
      "0                                Reprise de la session\\n   \n",
      "1      Je déclare reprise la session du Parlement eur...   \n",
      "2      Comme vous avez pu le constater, le grand \"bog...   \n",
      "3      Vous avez souhaité un débat à ce sujet dans le...   \n",
      "4      En attendant, je souhaiterais, comme un certai...   \n",
      "...                                                  ...   \n",
      "99995  Nous ne devrions pas nous satisfaire d'une sim...   \n",
      "99996                                Soyons courageux.\\n   \n",
      "99997  Légiférons, recherchons un niveau d'harmonisat...   \n",
      "99998  Veillons aux impacts de la circulation sur l'e...   \n",
      "99999  J'insiste également sur l'importance de légifé...   \n",
      "\n",
      "                                                      en  \n",
      "0                            Resumption of the session\\n  \n",
      "1      I declare resumed the session of the European ...  \n",
      "2      Although, as you will have seen, the dreaded '...  \n",
      "3      You have requested a debate on this subject in...  \n",
      "4      In the meantime, I should like to observe a mi...  \n",
      "...                                                  ...  \n",
      "99995  We must not be satisfied with a mere recommend...  \n",
      "99996                             We must now be bold.\\n  \n",
      "99997  Let us legislate, let us seek a greater degree...  \n",
      "99998  Let us be vigilant of the environmental impact...  \n",
      "99999  I would also like to reiterate the importance ...  \n",
      "\n",
      "[100000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Checking the resulting data frame\n",
    "print(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      fr  \\\n",
      "0                                Reprise de la session\\n   \n",
      "1      Je déclare reprise la session du Parlement eur...   \n",
      "2      Comme vous avez pu le constater, le grand \"bog...   \n",
      "3      Vous avez souhaité un débat à ce sujet dans le...   \n",
      "4      En attendant, je souhaiterais, comme un certai...   \n",
      "...                                                  ...   \n",
      "99995  Nous ne devrions pas nous satisfaire d'une sim...   \n",
      "99996                                Soyons courageux.\\n   \n",
      "99997  Légiférons, recherchons un niveau d'harmonisat...   \n",
      "99998  Veillons aux impacts de la circulation sur l'e...   \n",
      "99999  J'insiste également sur l'importance de légifé...   \n",
      "\n",
      "                                                      en  \\\n",
      "0                            Resumption of the session\\n   \n",
      "1      I declare resumed the session of the European ...   \n",
      "2      Although, as you will have seen, the dreaded '...   \n",
      "3      You have requested a debate on this subject in...   \n",
      "4      In the meantime, I should like to observe a mi...   \n",
      "...                                                  ...   \n",
      "99995  We must not be satisfied with a mere recommend...   \n",
      "99996                             We must now be bold.\\n   \n",
      "99997  Let us legislate, let us seek a greater degree...   \n",
      "99998  Let us be vigilant of the environmental impact...   \n",
      "99999  I would also like to reiterate the importance ...   \n",
      "\n",
      "                                                  en_ids  \\\n",
      "0                                [795, 8254, 6, 3, 1595]   \n",
      "1      [13, 3027, 3012, 3, 1595, 6, 3, 28, 54, 5443, ...   \n",
      "2      [1575, 4, 26, 57, 31, 23, 723, 4, 3, 1877, 646...   \n",
      "3      [25, 0, 470, 23, 1776, 10, 134, 17, 16, 332, 9...   \n",
      "4      [71, 3, 3406, 4, 13, 41, 66, 7, 3636, 10, 3150...   \n",
      "...                                                  ...   \n",
      "99995  [43, 46, 22, 18, 2366, 27, 10, 2946, 1453, 7, ...   \n",
      "99996                         [43, 46, 105, 18, 4935, 5]   \n",
      "99997  [647, 92, 5376, 4, 595, 92, 1559, 10, 420, 159...   \n",
      "99998  [647, 92, 18, 4205, 6, 3, 300, 751, 6, 966, 4,...   \n",
      "99999  [13, 38, 44, 66, 7, 2625, 3, 359, 6, 7496, 35,...   \n",
      "\n",
      "                                                  fr_ids  \n",
      "0                                [983, 8909, 5, 7, 1184]  \n",
      "1      [43, 2323, 1993, 7, 1184, 19, 59, 95, 20, 499,...  \n",
      "2      [526, 72, 493, 490, 10, 941, 4, 10, 339, 128, ...  \n",
      "3      [658, 493, 2515, 27, 163, 12, 26, 242, 21, 14,...  \n",
      "4      [111, 5133, 4, 37, 1393, 4, 74, 27, 510, 230, ...  \n",
      "...                                                  ...  \n",
      "99995  [57, 34, 717, 28, 22, 2881, 15, 3, 73, 962, 16...  \n",
      "99996                               [360, 2291, 7001, 6]  \n",
      "99997  [109, 117, 344, 1493, 117, 948, 4, 492, 2538, ...  \n",
      "99998  [1368, 40, 1185, 119, 46, 1976, 11, 5, 7, 1102...  \n",
      "99999  [141, 3, 4545, 88, 32, 8, 3, 715, 5, 5548, 32,...  \n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize our text (list of sentences) and\n",
    "# add it to our data frame in the column 'label'\n",
    "def tokenize_text(df, spm, text, label):\n",
    "    ids = []\n",
    "    for line in text:\n",
    "        id_line = spm.EncodeAsIds(line)\n",
    "        ids.append(id_line)\n",
    "    df[label] = ids\n",
    "\n",
    "# Let's run this function on the English text\n",
    "tokenize_text(training_df, en_sp, en_text, 'en_ids')\n",
    "# And on the French text\n",
    "tokenize_text(training_df, fr_sp, fr_text, 'fr_ids')\n",
    "\n",
    "# Checking the resulting data frame\n",
    "print(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence padding\n",
    "\n",
    "# Pad English tokens\n",
    "padded_en_ids = pad_sequences(\n",
    "    training_df['en_ids'].tolist(),\n",
    "    maxlen = max_sentence_length,\n",
    "    padding = 'post')\n",
    "# Add them to our training data frame\n",
    "training_df['pad_en_ids'] = padded_en_ids.tolist()\n",
    "\n",
    "# Pad French tokens\n",
    "padded_fr_ids = pad_sequences(\n",
    "    training_df['fr_ids'].tolist(),\n",
    "    maxlen = max_sentence_length,\n",
    "    padding = 'post')\n",
    "# Add them to our training data frame\n",
    "training_df['pad_fr_ids'] = padded_fr_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [983, 8909, 5, 7, 1184, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "1        [43, 2323, 1993, 7, 1184, 19, 59, 95, 20, 499,...\n",
      "2        [526, 72, 493, 490, 10, 941, 4, 10, 339, 128, ...\n",
      "3        [658, 493, 2515, 27, 163, 12, 26, 242, 21, 14,...\n",
      "4        [111, 5133, 4, 37, 1393, 4, 74, 27, 510, 230, ...\n",
      "                               ...                        \n",
      "99985    [56, 10, 71, 4, 197, 7, 276, 4, 37, 115, 75, 1...\n",
      "99986    [114, 3034, 9, 10, 230, 537, 94, 1358, 5, 745,...\n",
      "99987    [114, 624, 47, 611, 9, 1811, 9, 10, 61, 29, 24...\n",
      "99988    [68, 551, 5, 7, 42, 32, 14, 1007, 5, 7, 179, 6...\n",
      "99989    [43, 1498, 144, 2025, 32, 14, 807, 20, 1753, 7...\n",
      "Name: pad_fr_ids, Length: 99990, dtype: object\n",
      "99990    [533, 230, 1358, 5, 745, 15, 3, 5739, 47, 14, ...\n",
      "99991    [519, 1232, 4, 36, 771, 81, 23, 13, 624, 6614,...\n",
      "99992    [888, 2202, 4230, 67, 3216, 16, 7, 561, 13, 74...\n",
      "99993    [442, 237, 249, 5, 64, 3034, 51, 4160, 12, 64,...\n",
      "99994    [1772, 132, 15, 3, 463, 1669, 32, 7, 2272, 9, ...\n",
      "99995    [57, 34, 717, 28, 22, 2881, 15, 3, 73, 962, 16...\n",
      "99996    [360, 2291, 7001, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "99997    [948, 4, 492, 2538, 655, 119, 27, 188, 15, 3, ...\n",
      "99998    [1368, 40, 1185, 119, 46, 1976, 11, 5, 7, 1102...\n",
      "99999    [141, 3, 4545, 88, 32, 8, 3, 715, 5, 5548, 32,...\n",
      "Name: pad_fr_ids, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(training_df['pad_fr_ids'][0:-10])\n",
    "print(training_df['pad_fr_ids'][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our training input and target output numpy array to feed to our NMT model\n",
    "# We'll take the first 9,990 lines for training\n",
    "trainX = np.asarray(training_df['pad_fr_ids'][0:-10].tolist())\n",
    "trainY = np.asarray(training_df['pad_en_ids'][0:-10].tolist())\n",
    "# Reshape the output to match expected dimensionality\n",
    "trainY = trainY.reshape(trainY.shape[0], trainY.shape[1], 1)\n",
    "\n",
    "# The test dataset is just for a visual check, so we'll only take the last 10 lines\n",
    "testX = np.asarray(training_df['pad_fr_ids'][-10:].tolist())\n",
    "testY = np.asarray(training_df['pad_en_ids'][-10:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99990, 100)\n",
      "(99990, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training our NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 1024)         10240000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 1024)         8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 10000)        10250000  \n",
      "=================================================================\n",
      "Total params: 37,275,408\n",
      "Trainable params: 37,275,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Creating a Keras Sequential object for our NMT model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    en_vocab_size,\n",
    "    nb_cells,\n",
    "    input_length = max_sentence_length))\n",
    "# Adding an LSTM layer to act as the encoder\n",
    "model.add(LSTM(\n",
    "    units = nb_cells,\n",
    "    return_sequences = False))\n",
    "# Since we are not returning a sequence but just a vector, we need\n",
    "# to repeat this vector multiple times to input it to our decoder LSTM\n",
    "model.add(RepeatVector(max_sentence_length))\n",
    "# Adding an LSTM layer to act as the decoder\n",
    "model.add(LSTM(\n",
    "    units = nb_cells,\n",
    "    return_sequences = True))\n",
    "# Adding a softmax\n",
    "model.add((Dense(\n",
    "    fr_vocab_size,\n",
    "    activation = 'softmax')))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luc/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "45888/99990 [============>.................] - ETA: 5:14:17 - loss: 2.2840"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2deb2c12318c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0;31m# Delegate logic to `fit_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         return training_arrays.fit_loop(self, fit_function, fit_inputs,\n\u001b[0m\u001b[1;32m   1228\u001b[0m                                         \u001b[0mout_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs = nb_epochs,\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our trained model\n",
    "model.save(trained_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the trained model\n",
    "model = load_model(trained_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(testX)\n",
    "\n",
    "decoded_predictions = []\n",
    "for index in range(10):\n",
    "    print(\"Expected:\")\n",
    "    print(en_sp.DecodeIds(testY[index].tolist()))\n",
    "    print(\"Predicted:\")\n",
    "    print(en_sp.DecodeIds(predictions[index, :].tolist()))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
