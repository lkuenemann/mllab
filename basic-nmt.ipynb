{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus:\n",
    "French-English sentence pairs by the Tatoeba Project:\n",
    "http://www.manythings.org/anki/\n",
    "\n",
    "Tokenizer:\n",
    "SentencePiece 0.1.91\n",
    "https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# General modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Tokenizer\n",
    "import sentencepiece as sp\n",
    "# Data preprocessing tools\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# General Keras functionalities\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Neural network components from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source text database\n",
    "data_filename = \"./fra.txt\"\n",
    "\n",
    "# Text preprocessing\n",
    "subset_size = 10000 # total sentence pairs in dataset: 175,623\n",
    "test_size = 1000 # sentence pairs for testing\n",
    "train_size = subset_size - test_size # sentence pairs for training\n",
    "clean_en_filename = \"clean_en.txt\" # file to save cleaned English text data\n",
    "clean_fr_filename = \"clean_fr.txt\" # French\n",
    "clean_train_filename = \"clean_en_fr.txt\" # both languages\n",
    "\n",
    "# Let's define vocabulary size for out tokenizer\n",
    "vocab_size = 2000\n",
    "# We'll use the same size for both languages to simplify\n",
    "en_vocab_size = vocab_size\n",
    "fr_vocab_size = vocab_size\n",
    "# max_sentence_length = 20 # Pad all sentences to 40 word pieces (tokens) max\n",
    "\n",
    "# Defining all the parameters of our network\n",
    "nb_cells = 256 # LSTM cells in encoder/decoder\n",
    "\n",
    "# Training parameters\n",
    "nb_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "# File name to save our trained model weights\n",
    "trained_model_filename = \"fr_en_nmt_model_test.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the text dataset file\n",
    "file = open(\n",
    "    data_filename,\n",
    "    mode = 'rt',\n",
    "    encoding = 'utf-8')\n",
    "# Getting the text content\n",
    "raw_text = file.read()\n",
    "# Closing the file handle\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
      "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (g\n"
     ]
    }
   ],
   "source": [
    "# Checking the beginning of the text\n",
    "print(raw_text[:256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing: cleaning and reducing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset in a Pandas dataframe\n",
    "train_df = pd.read_csv(\n",
    "    data_filename, # path to our dataset file\n",
    "    sep='\\t', # tab delimiter between columns in the csv\n",
    "    usecols=[0, 1], # import only columns 0 and 1\n",
    "    nrows=subset_size, # read only the first subset_size rows\n",
    "    names=[\"en\",\"fr\"]) # label them 'en' and 'fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    en                                fr\n",
      "0                  Go.                              Va !\n",
      "1                  Hi.                           Salut !\n",
      "2                  Hi.                            Salut.\n",
      "3                 Run!                           Cours !\n",
      "4                 Run!                          Courez !\n",
      "...                ...                               ...\n",
      "9995  Be more precise.                 Soit plus précis.\n",
      "9996  Be quiet, girls.  Restez tranquilles, les filles !\n",
      "9997  Be very careful.               Sois très prudent !\n",
      "9998  Be very careful.              Soyez très prudent !\n",
      "9999  Be very careful.             Soyez très prudente !\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check our dataframe\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional cleanup here\n",
    "# TODO: maybe lowercase all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our cleaned and reduced dataset\n",
    "train_df.to_csv(\n",
    "    clean_train_filename,\n",
    "    sep='\\t', # using tab separators\n",
    "    index=False) # don't print the row index in the csv\n",
    "\n",
    "# Saving the English part separately for SentencePiece\n",
    "train_df.to_csv(\n",
    "    clean_en_filename,\n",
    "    columns=['en'], # print only the column 'en'\n",
    "    index=False)\n",
    "\n",
    "# And the French one\n",
    "train_df.to_csv(\n",
    "    clean_fr_filename,\n",
    "    columns=['fr'], # print only the column 'fr'\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for English\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input = clean_en_filename,\n",
    "    model_prefix = 'en',\n",
    "    vocab_size = en_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for French\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input = clean_fr_filename,\n",
    "    model_prefix = 'fr',\n",
    "    vocab_size = fr_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tokenizer object for English\n",
    "en_sp = sp.SentencePieceProcessor()\n",
    "# Loading the English model\n",
    "en_sp.Load(\"en.model\")\n",
    "# Creating a tokenizer object for French\n",
    "fr_sp = sp.SentencePieceProcessor()\n",
    "# Loading the French model\n",
    "fr_sp.Load(\"fr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', '▁like', '▁gree', 'n', '▁a', 'pples', '.']\n",
      "[4, 41, 1033, 24, 10, 1235, 3]\n",
      "I like green apples.\n",
      "['▁J', \"'\", 'aime', '▁le', 's', '▁pommes', '▁vert', 'es', '.']\n",
      "[11, 4, 79, 20, 5, 1449, 1155, 63, 3]\n",
      "J'aime les pommes vertes.\n"
     ]
    }
   ],
   "source": [
    "# Testing the English tokenizer\n",
    "en_test_sentence = \"I like green apples.\"\n",
    "# Encoding pieces\n",
    "print(en_sp.EncodeAsPieces(en_test_sentence))\n",
    "# Encoding pieces as IDs\n",
    "print(en_sp.EncodeAsIds(en_test_sentence))\n",
    "# Decoding encoded IDs\n",
    "print(en_sp.DecodeIds(en_sp.EncodeAsIds(en_test_sentence)))\n",
    "\n",
    "# Testing the French tokenizer\n",
    "fr_test_sentence = \"J'aime les pommes vertes.\"\n",
    "# Encoding pieces\n",
    "print(fr_sp.EncodeAsPieces(fr_test_sentence))\n",
    "# Encoding pieces as IDs\n",
    "print(fr_sp.EncodeAsIds(fr_test_sentence))\n",
    "# Decoding encoded IDs\n",
    "print(fr_sp.DecodeIds(fr_sp.EncodeAsIds(fr_test_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained tokenizer for English and French\n",
    "# Creating a tokenizer object for English\n",
    "en_sp = sp.SentencePieceProcessor()\n",
    "# Loading the English model\n",
    "en_sp.Load(\"en.model\")\n",
    "# Creating a tokenizer object for French\n",
    "fr_sp = sp.SentencePieceProcessor()\n",
    "# Loading the French model\n",
    "fr_sp.Load(\"fr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned up dataset\n",
    "train_df = pd.read_csv(\n",
    "    clean_train_filename,\n",
    "    sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    en                                fr\n",
      "0                  Go.                              Va !\n",
      "1                  Hi.                           Salut !\n",
      "2                  Hi.                            Salut.\n",
      "3                 Run!                           Cours !\n",
      "4                 Run!                          Courez !\n",
      "...                ...                               ...\n",
      "9995  Be more precise.                 Soit plus précis.\n",
      "9996  Be quiet, girls.  Restez tranquilles, les filles !\n",
      "9997  Be very careful.               Sois très prudent !\n",
      "9998  Be very careful.              Soyez très prudent !\n",
      "9999  Be very careful.             Soyez très prudente !\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Checking the resulting data frame\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize our text (list of sentences) and\n",
    "# add it to our data frame in the column 'label'\n",
    "def tokenize_text(df, spm, txt_label, id_label):\n",
    "    ids = []\n",
    "    for line in df[txt_label].tolist():\n",
    "        id_line = spm.EncodeAsIds(line)\n",
    "        ids.append(id_line)\n",
    "    df[id_label] = ids\n",
    "\n",
    "# Let's run this function on the English text\n",
    "tokenize_text(train_df, en_sp, 'en', 'en_ids')\n",
    "# And on the French text\n",
    "tokenize_text(train_df, fr_sp, 'fr', 'fr_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    en                                fr  \\\n",
      "0                  Go.                              Va !   \n",
      "1                  Hi.                           Salut !   \n",
      "2                  Hi.                            Salut.   \n",
      "3                 Run!                           Cours !   \n",
      "4                 Run!                          Courez !   \n",
      "...                ...                               ...   \n",
      "9995  Be more precise.                 Soit plus précis.   \n",
      "9996  Be quiet, girls.  Restez tranquilles, les filles !   \n",
      "9997  Be very careful.               Sois très prudent !   \n",
      "9998  Be very careful.              Soyez très prudent !   \n",
      "9999  Be very careful.             Soyez très prudente !   \n",
      "\n",
      "                      en_ids                               fr_ids  \n",
      "0                    [81, 3]                             [199, 9]  \n",
      "1                  [1004, 3]                             [992, 9]  \n",
      "2                  [1004, 3]                             [992, 3]  \n",
      "3                  [472, 18]                      [18, 812, 5, 9]  \n",
      "4                  [472, 18]                     [18, 812, 49, 9]  \n",
      "...                      ...                                  ...  \n",
      "9995  [42, 320, 282, 919, 3]              [118, 22, 203, 1143, 3]  \n",
      "9996   [42, 271, 89, 753, 3]  [6, 221, 265, 5, 66, 20, 5, 765, 9]  \n",
      "9997   [42, 17, 151, 177, 3]            [118, 5, 58, 169, 361, 9]  \n",
      "9998   [42, 17, 151, 177, 3]           [108, 49, 58, 169, 361, 9]  \n",
      "9999   [42, 17, 151, 177, 3]           [108, 49, 58, 169, 375, 9]  \n",
      "\n",
      "[10000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Checking the resulting data frame\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tokenized English sentence length\n",
    "en_max_len = max(len(line) for line in train_df['en_ids'].tolist())\n",
    "# Check tokenized French sentence length\n",
    "fr_max_len = max(len(line) for line in train_df['fr_ids'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English maximum sentence length: 10\n",
      "French maximum sentence length: 22\n"
     ]
    }
   ],
   "source": [
    "print(\"English maximum sentence length:\", en_max_len)\n",
    "print(\"French maximum sentence length:\", fr_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence padding\n",
    "# Pad English tokens\n",
    "padded_en_ids = pad_sequences(\n",
    "    train_df['en_ids'].tolist(),\n",
    "    maxlen = en_max_len,\n",
    "    padding = 'post')\n",
    "# Add them to our training data frame\n",
    "train_df['pad_en_ids'] = padded_en_ids.tolist()\n",
    "\n",
    "# Pad French tokens\n",
    "padded_fr_ids = pad_sequences(\n",
    "    train_df['fr_ids'].tolist(),\n",
    "    maxlen = fr_max_len,\n",
    "    padding = 'post')\n",
    "# Add them to our training data frame\n",
    "train_df['pad_fr_ids'] = padded_fr_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [199, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "1       [992, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "2       [992, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "3       [18, 812, 5, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4       [18, 812, 49, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
      "                              ...                        \n",
      "9995    [118, 22, 203, 1143, 3, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "9996    [6, 221, 265, 5, 66, 20, 5, 765, 9, 0, 0, 0, 0...\n",
      "9997    [118, 5, 58, 169, 361, 9, 0, 0, 0, 0, 0, 0, 0,...\n",
      "9998    [108, 49, 58, 169, 361, 9, 0, 0, 0, 0, 0, 0, 0...\n",
      "9999    [108, 49, 58, 169, 375, 9, 0, 0, 0, 0, 0, 0, 0...\n",
      "Name: pad_fr_ids, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df['pad_fr_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling our dataframe around\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [19, 5, 357, 76, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
      "1       [11, 4, 103, 142, 145, 128, 93, 3, 0, 0, 0, 0,...\n",
      "2       [6, 1367, 5, 66, 119, 90, 215, 9, 0, 0, 0, 0, ...\n",
      "3       [14, 77, 4, 189, 52, 4, 172, 448, 3, 0, 0, 0, ...\n",
      "4       [19, 5, 24, 4, 281, 584, 3, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "9995    [118, 5, 937, 16, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "9996    [451, 1829, 23, 146, 3, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "9997    [102, 768, 293, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9998    [29, 26, 59, 359, 25, 54, 3, 0, 0, 0, 0, 0, 0,...\n",
      "9999    [19, 5, 543, 100, 153, 322, 76, 3, 0, 0, 0, 0,...\n",
      "Name: pad_fr_ids, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df['pad_fr_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our training input and target output numpy array to feed to our NMT model\n",
    "# We'll take the first train_size lines for training (after random shuffle)\n",
    "trainX = np.asarray(train_df['pad_fr_ids'][0:train_size].tolist())\n",
    "trainY = np.asarray(train_df['pad_en_ids'][0:train_size].tolist())\n",
    "# Reshape the output to match expected dimensionality\n",
    "trainY = trainY.reshape(trainY.shape[0], trainY.shape[1], 1)\n",
    "\n",
    "# The test dataset for checking on the last test_size lines (after random shuffle)\n",
    "testX = np.asarray(train_df['pad_fr_ids'][train_size:].tolist())\n",
    "testY = np.asarray(train_df['pad_en_ids'][train_size:].tolist())\n",
    "# Reshape the output to match expected dimensionality\n",
    "testY = testY.reshape(testY.shape[0], testY.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 22)\n",
      "(9000, 10, 1)\n",
      "(1000, 22)\n",
      "(1000, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training our NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 22, 256)           512000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10, 2000)          514000    \n",
      "=================================================================\n",
      "Total params: 2,076,624\n",
      "Trainable params: 2,076,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Creating a Keras Sequential object for our NMT model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer to map our one-hot encoding to a small word space\n",
    "model.add(Embedding(\n",
    "    fr_vocab_size,\n",
    "    nb_cells,\n",
    "    input_length = fr_max_len,\n",
    "    mask_zero = True))\n",
    "# Adding an LSTM layer to act as the encoder\n",
    "model.add(LSTM(\n",
    "    units = nb_cells,\n",
    "    return_sequences = False))\n",
    "# Since we are not returning a sequence but just a vector, we need\n",
    "# to repeat this vector multiple times to input it to our decoder LSTM\n",
    "model.add(RepeatVector(en_max_len))\n",
    "# Adding an LSTM layer to act as the decoder\n",
    "model.add(LSTM(\n",
    "    units = nb_cells,\n",
    "    return_sequences = True))\n",
    "# Adding a softmax\n",
    "model.add((Dense(\n",
    "    en_vocab_size,\n",
    "    activation = 'softmax')))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luc/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 3.2742 - val_loss: 2.5295\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.52945, saving model to model.h5\n",
      "Epoch 2/30\n",
      "9000/9000 [==============================] - 41s 5ms/step - loss: 2.4009 - val_loss: 2.3331\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.52945 to 2.33310, saving model to model.h5\n",
      "Epoch 3/30\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 2.2398 - val_loss: 2.2093\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.33310 to 2.20935, saving model to model.h5\n",
      "Epoch 4/30\n",
      "9000/9000 [==============================] - 48s 5ms/step - loss: 2.1192 - val_loss: 2.1129\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.20935 to 2.11291, saving model to model.h5\n",
      "Epoch 5/30\n",
      "9000/9000 [==============================] - 41s 5ms/step - loss: 2.0196 - val_loss: 2.0346\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.11291 to 2.03464, saving model to model.h5\n",
      "Epoch 6/30\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 1.9041 - val_loss: 1.9161\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.03464 to 1.91607, saving model to model.h5\n",
      "Epoch 7/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.7815 - val_loss: 1.8392\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.91607 to 1.83921, saving model to model.h5\n",
      "Epoch 8/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 1.6864 - val_loss: 1.7748\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.83921 to 1.77480, saving model to model.h5\n",
      "Epoch 9/30\n",
      "9000/9000 [==============================] - 50s 6ms/step - loss: 1.6024 - val_loss: 1.7275\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.77480 to 1.72754, saving model to model.h5\n",
      "Epoch 10/30\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 1.5240 - val_loss: 1.6686\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.72754 to 1.66864, saving model to model.h5\n",
      "Epoch 11/30\n",
      "9000/9000 [==============================] - 40s 4ms/step - loss: 1.4451 - val_loss: 1.6176\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.66864 to 1.61763, saving model to model.h5\n",
      "Epoch 12/30\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 1.3700 - val_loss: 1.5780\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.61763 to 1.57795, saving model to model.h5\n",
      "Epoch 13/30\n",
      "9000/9000 [==============================] - 40s 4ms/step - loss: 1.3049 - val_loss: 1.5454\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.57795 to 1.54542, saving model to model.h5\n",
      "Epoch 14/30\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 1.2404 - val_loss: 1.5174\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.54542 to 1.51735, saving model to model.h5\n",
      "Epoch 15/30\n",
      "9000/9000 [==============================] - 47s 5ms/step - loss: 1.1805 - val_loss: 1.4927\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.51735 to 1.49268, saving model to model.h5\n",
      "Epoch 16/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.1211 - val_loss: 1.4543\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.49268 to 1.45427, saving model to model.h5\n",
      "Epoch 17/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 1.0578 - val_loss: 1.4556\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.45427\n",
      "Epoch 18/30\n",
      "9000/9000 [==============================] - 39s 4ms/step - loss: 1.0064 - val_loss: 1.4133\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.45427 to 1.41332, saving model to model.h5\n",
      "Epoch 19/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 0.9505 - val_loss: 1.4132\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.41332 to 1.41324, saving model to model.h5\n",
      "Epoch 20/30\n",
      "9000/9000 [==============================] - 36s 4ms/step - loss: 0.8979 - val_loss: 1.3847\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.41324 to 1.38466, saving model to model.h5\n",
      "Epoch 21/30\n",
      "9000/9000 [==============================] - 36s 4ms/step - loss: 0.8508 - val_loss: 1.3584\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.38466 to 1.35840, saving model to model.h5\n",
      "Epoch 22/30\n",
      "9000/9000 [==============================] - 37s 4ms/step - loss: 0.8021 - val_loss: 1.3487\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.35840 to 1.34869, saving model to model.h5\n",
      "Epoch 23/30\n",
      "9000/9000 [==============================] - 40s 4ms/step - loss: 0.7542 - val_loss: 1.3382\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.34869 to 1.33816, saving model to model.h5\n",
      "Epoch 24/30\n",
      "9000/9000 [==============================] - 41s 5ms/step - loss: 0.7108 - val_loss: 1.3206\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.33816 to 1.32056, saving model to model.h5\n",
      "Epoch 25/30\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.6673 - val_loss: 1.3166\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.32056 to 1.31656, saving model to model.h5\n",
      "Epoch 26/30\n",
      "9000/9000 [==============================] - 37s 4ms/step - loss: 0.6194 - val_loss: 1.3067\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.31656 to 1.30670, saving model to model.h5\n",
      "Epoch 27/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 0.5814 - val_loss: 1.3067\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.30670 to 1.30669, saving model to model.h5\n",
      "Epoch 28/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 0.5485 - val_loss: 1.2920\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.30669 to 1.29204, saving model to model.h5\n",
      "Epoch 29/30\n",
      "9000/9000 [==============================] - 38s 4ms/step - loss: 0.5146 - val_loss: 1.2916\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.29204 to 1.29160, saving model to model.h5\n",
      "Epoch 30/30\n",
      "9000/9000 [==============================] - 35s 4ms/step - loss: 0.4845 - val_loss: 1.2945\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.29160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f929c35bd60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model_filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_filename,\n",
    "    monitor = 'val_loss',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    mode = 'min')\n",
    "model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs = nb_epochs,\n",
    "    batch_size = batch_size,\n",
    "    callbacks = [checkpoint],\n",
    "    validation_data = (testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the trained model\n",
    "model = load_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "J'adore l'été. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "I love summer. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "I love hi. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Je viens juste de déménager. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "I just moved. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "I'm starved. ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Je reviens. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "I'll come back. ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "I'll be back. ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Ça fonctionnait. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "It was working. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "It was it. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "C'est elle. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "That's her. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "That's all. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Vous aimeriez Tom. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "You'd like Tom. ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "K loves Tom. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Je t'ai suivi. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "I followed you. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "I followed you. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Nous sommes fatiguées. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "We're tired. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "We're tired. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Vous êtes rusée. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "You're crafty. ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "You're crafty. ⁇  ⁇  ⁇ \n",
      "\n",
      "Original:\n",
      "Apportez de la nourriture. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Expected:\n",
      "Bring food. ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Predicted:\n",
      "Bring the came.. ⁇  ⁇  ⁇  ⁇ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(testX)\n",
    "\n",
    "# Check the translation on a few sentences\n",
    "decoded_predictions = []\n",
    "for index in range(10):\n",
    "    print(\"Original:\")\n",
    "    print(fr_sp.DecodeIds(testX[index, :].tolist()))\n",
    "    print(\"Expected:\")\n",
    "    print(en_sp.DecodeIds(testY[index, :, 0].tolist()))\n",
    "    print(\"Predicted:\")\n",
    "    print(en_sp.DecodeIds(predictions[index, :].tolist()))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
